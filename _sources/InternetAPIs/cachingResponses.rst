..  Copyright (C)  Jackie Cohen, Paul Resnick.  Permission is granted to copy, distribute
    and/or modify this document under the terms of the GNU Free Documentation
    License, Version 1.3 or any later version published by the Free Software
    Foundation; with Invariant Sections being Forward, Prefaces, and
    Contributor List, no Front-Cover Texts, and no Back-Cover Texts.  A copy of
    the license is included in the section entitled "GNU Free Documentation
    License".

.. _caching_responses:

Caching Response Content
========================

You haven't experienced it yet, but if you get complicated data back from a REST API, it may take you many tries to compose and debug code that *processes* that data in the way that you want. (See the :ref:`Nested Data chapter<nested_chap>`.) It is a good practice, for many reasons, not to keep contacting a REST API to re-request the same data every time you run your program.

To avoid re-requesting the same data, we will use a programming pattern known as **caching**. It works like this:

1. Before doing some expensive operation (like calling ``requests.get`` to get data from a REST API), check whether you have already saved ("cached") the results that would be generated by making that request.
2. If so, return that same data
3. If not, perform the expensive operation and save ("cache") the results (e.g. the complicated data) in your cache file so you won't have to perform it again the next time.

There are three reasons why caching is a good idea during your
software development using REST APIs:

* It reduces load on the website that is providing you data. It is always nice to be courteous when using other people's resources. Moreover, some websites impose rate limits: for example, after 15 requests in a 15 minute period, the site may start sending error responses. That will be confusing and annoying for you.
* It will make your program run faster. Connections over the Internet can take a few seconds, or even tens of seconds, if you are requesting a lot of data. It might not seem like much, but debugging is a lot easier when you can make a change in your program, run it, and get an almost instant response.
* It is harder to debug the code that processes complicated data if the content that is coming back can change on each run of your code. It's amazing to be able to write programs that fetch real-time data like available iTunes podcasts or the latest tweets from Twitter. But it can be hard to debug that code if you are having problems that only occur on certain Tweets (e.g. those in foreign languages). When you encounter problematic data, it's helpful if you save a copy and can debug your program working on that saved, static copy of the data.

There are some downsides to caching data -- for example, if you always want to find out when data has changed, and your default is to rely on already-cached data, then you have a problem. However, when you're working on developing code that will work, caching is worth the tradeoff.

In this book, we are providing a pattern we're calling the "caching pattern" in order to perform this caching operation.

This is not the only way to perform caching in a program. (In fact, if you go on to learn about web development, you'll find that you encounter caching all the time -- if you've ever had the experience of seeing old data when you go to a website and thinking, "Huh, that's weird, it should really be different now... why am I still seeing that?" that happens because your web browser is performing a kind of caching operation on data from the internet.)

However, in this book, we'll treat this as the default caching pattern for now.

We will use a python dictionary to store the results of expensive operations (the calls to ``requests.get()``).

Each invocation of ``requests.get`` might have a different URL. Each different URL represents the data that is being requested. For example, you might have a URL that points to data about "all the words that rhyme with the word rain" or "all the words that rhyme with the word orange". You might have a URL that points to "20 photos tagged with the word 'river'", another that points to "50 photos tagged with the word 'river'", and yet another that points to "20 photos tagged with the words 'river' and 'mountains'". Etc.

When you cache data, you want to check whether you have already gotten data that corresponds to the request you're making. If so, just use that data for processing in your program. If not, go make the request, retrieve data for it, and cache it -- save it, so the next time you make exactly the same request, you'll already have data, and won't have to make another expensive operation happen (e.g. going to make a request to the internet). Making a request to a REST API on the internet is more "expensive" -- takes more time -- than getting data out of a file that's on your computer.

In order to make this happen, we'll use a couple pieces of code:

* a pattern at the beginning of a program file that will use caching, that we'll call our "caching pattern setup"
* a helper function
* finally, we'll change the structure of functions that make requests for data so that they don't just go grab new data -- they also rely on caching

Below is an example of the caching pattern setup and a function that uses the caching pattern to get data from the Datamuse API. In the next section, we'll break down what this code is doing.

.. sourcecode:: python

    import requests
    import json

    CACHE_FNAME = 'cache_file_name.json'
    try:
        cache_file = open(CACHE_FNAME, 'r')
        cache_contents = cache_file.read()
        _cache_diction = json.loads(cache_contents)
        cache_file.close()
    except: # But if anything doesn't work,
        _cache_diction = {}

    # A helper function that accepts 3 parameters, 2 required, and returns a string that uniquely represents the request that could be made with this info
    def params_unique_combination(baseurl, params_d, private_keys=["api_key"]):
        alphabetized_keys = sorted(params_d.keys())
        res = []
        for k in alphabetized_keys:
            if k not in private_keys:
                res.append("{}-{}".format(k, params_d[k]))
        return baseurl + "_".join(res)

    # Function to get data from the datamuse API, using this caching pattern
    def get_from_datamuse_caching(rhymes_with):
        baseurl = "https://api.datamuse.com/words"
        params_diction = {}
        params_diction["rel_rhy"] = rhymes_with
        unique_ident = params_unique_combination(baseurl,params_diction)
        if unique_ident in _cache_diction:
            print("Getting cached data...")
            return _cache_diction[unique_ident]
        else:
            print("Making a request for new data...")
            # Make the request and cache the new data
            resp = requests.get(baseurl, params_diction)
            _cache_diction[unique_ident] = json.loads(resp.text)
            dumped_json_cache = json.dumps(_cache_diction)
            fw = open(CACHE_FNAME,"w")
            fw.write(dumped_json_cache)
            fw.close() # Close the open file
            return _cache_diction[unique_ident]
